{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPVdpld5N5pj83xcvNkmXy0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/azamjon98/final_project/blob/main/Customer_churn_telecom_data_set.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nouMIOvUgzKq",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, classification_report,roc_auc_score, accuracy_score, f1_score, precision_score, recall_score, ConfusionMatrixDisplay\n",
        "from tensorflow.keras.layers import Normalization,Dense, InputLayer\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "!pip install bayesian-optimization\n",
        "from bayes_opt import BayesianOptimization\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url_link='https://github.com/myasmin/Teleco-Churn-Data-Analysis/raw/main/Telco_customer_churn.xlsx'\n",
        "df=pd.read_excel(url_link)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "q1K5-RqVf1RG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We should drop columns including  `'CustomerID'`,`'Churn Score'`,`'CLTV'`,`'Churn Reason'`. Because 1st one expresses just id of the customer and the othersexpresses exit interview results.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BSHonnSwTZiP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(columns=['CustomerID','Churn Score','CLTV','Churn Reason'],inplace=True)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "2VCa2TSoEJsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Count'].unique(), df['State'].unique(), df['Country'].unique()"
      ],
      "metadata": {
        "id": "ONuSdQtDEvDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We can also drop columns consist of single values: `'Count','Country','State','Lat Long'`"
      ],
      "metadata": {
        "id": "8cuMQqGwT50o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(columns=['Count','Country','State','Lat Long'],inplace=True)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "SgkLWpIzE6hY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Column `Churn Label` is the same as `Churn Value` so we can drop it also"
      ],
      "metadata": {
        "id": "AQ2uQwmWUKSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(columns=['Churn Label'],inplace=True)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "tFqYNqADFju4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We extract numerical and categorical data"
      ],
      "metadata": {
        "id": "y27D1XckUYNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_cat=df.select_dtypes(include='object')\n",
        "df_cat.head()"
      ],
      "metadata": {
        "id": "SP1XYXwJFTHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `Column Total Charges` must be numerical we convert into numerical values."
      ],
      "metadata": {
        "id": "ibnWoSm6Ufc9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_cat['Total Charges']=df_cat['Total Charges'].replace(\" \",0,regex=True)\n",
        "df_cat['Total Charges']=pd.to_numeric(df_cat['Total Charges'])\n",
        "df_cat.head()"
      ],
      "metadata": {
        "id": "r7nboyqpGAFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### we have coordinates of the customer so that we do not need `Zip Code` of their address"
      ],
      "metadata": {
        "id": "tIFIHmG7Uwpf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_num=df.select_dtypes(exclude='object')\n",
        "df_num.drop(columns=['Zip Code'],inplace=True)\n",
        "df_num.head()"
      ],
      "metadata": {
        "id": "PyY5Nqj9FZPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.concat([df_cat,df_num],axis=1)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "dLPS1GuuFbig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We should replace all free spaces `\" \"` with `'_'`**"
      ],
      "metadata": {
        "id": "4_r5DLdXU_Qo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns=df.columns.str.replace(' ','_')"
      ],
      "metadata": {
        "id": "3HOJCE_zIbPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['City']=df['City'].str.replace(' ','_')\n",
        "df['Internet Service']=df['Internet Service'].str.replace(' ','_')\n",
        "df['Payment Method']=df['Payment Method'].str.replace(' ','_')\n",
        "df['Contract']=df['Contract'].str.replace(' ','_')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "YLzz1F_JIr1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Specify the features and the target values"
      ],
      "metadata": {
        "id": "GyM8hK9DVSzL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X=df.drop(columns=['Churn_Value'])\n",
        "y=df['Churn_Value']"
      ],
      "metadata": {
        "id": "8sVgCAIaKQUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**For categorical data we use `One-Hot Encoding`**"
      ],
      "metadata": {
        "id": "d22pIgdUV9th"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X=pd.get_dummies(X,drop_first=True)\n",
        "X.head()"
      ],
      "metadata": {
        "id": "-EMxnu2IWBSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.value_counts(normalize=True)"
      ],
      "metadata": {
        "id": "EWBgtYOTKctR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We have imbalanced data so that we should use stratified sampling**"
      ],
      "metadata": {
        "id": "Q0T6F8eqWFC8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We are going to train with deep neural networks and compare the results with classical ML algorithm**"
      ],
      "metadata": {
        "id": "F_0P_7ThWSj-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1,stratify=y, random_state=42) # divide into test and train sets\n",
        "\n",
        "X_train=tf.constant(X_train) # convert into tensor values\n",
        "y_train=tf.constant(y_train)\n",
        "\n",
        "normalizer=Normalization(axis=-1) # normalize them they bust be in the same range\n",
        "norm_array=tf.constant([np.arange(3,1180),  # data has 1177 features so we need numbers between 3 and 1180\n",
        "                        np.arange(4,1181)]\n",
        "                        )\n",
        "\n",
        "normalizer.adapt(norm_array)\n",
        "normalizer(norm_array)\n",
        "normalizer.adapt(X_train)\n",
        "X_train=normalizer(X_train)\n",
        "\n",
        "def create_model():  # model function consist of input, normaliation, and hidden dense layers with activation which is relu\n",
        "    normalizer = Normalization(axis=-1)\n",
        "    normalizer.adapt(X_train)\n",
        "\n",
        "    model = tf.keras.Sequential([\n",
        "        InputLayer(input_shape=(1177,)),\n",
        "        normalizer,\n",
        "        Dense(256, activation='relu'),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(1,activation='sigmoid')  # output layer with activation which is sigmoid\n",
        "    ])\n",
        "\n",
        "\n",
        "\n",
        "    model.compile(optimizer='adam',    # we use adam optimizer and target is binary classification so we use binary_crossentropy as a loss function\n",
        "                 loss='binary_crossentropy',\n",
        "                  metrics=['AUC']) # data is imbalanced so using AUC is better\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create and train model with negative MSE loss\n",
        "model_clf = create_model()\n",
        "print(model_clf.summary())"
      ],
      "metadata": {
        "id": "Nopff4XrPjo-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model in 20 epochs\n",
        "history = model_clf.fit(X_train, y_train,\n",
        "                   epochs=20,\n",
        "                   batch_size=32,\n",
        "                   validation_split=0.2,\n",
        "                   verbose=1) # verbose 1 means it shows all results in each epoch"
      ],
      "metadata": {
        "collapsed": true,
        "id": "6F3CPDZvQrRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Check the model with test data**"
      ],
      "metadata": {
        "id": "v9k92kzlX1rg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test=tf.constant(X_test)\n",
        "y_test=tf.constant(y_test)\n",
        "\n",
        "normalizer.adapt(X_test)\n",
        "X_test=normalizer(X_test)\n",
        "\n",
        "pred2=np.round(model_clf.predict(X_test))\n",
        "cm=confusion_matrix(y_test,pred2)\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "scores = {\n",
        "        'ROC-AUC': roc_auc_score(y_test, pred2),\n",
        "        'Accuracy': accuracy_score(y_test, pred2),\n",
        "        'F1': f1_score(y_test, pred2),\n",
        "        'Precision': precision_score(y_test, pred2),\n",
        "        'Sensitivity (TPR / Recall)': recall_score(y_test, pred2),\n",
        "        'Specificity (TNR)': tn / (tn+fp)\n",
        "    }\n",
        "\n",
        "print('Testing Set Scores:')\n",
        "for metric, score in scores.items():\n",
        "    print(f'- {metric}: {score:.4f}')\n",
        "    print()\n",
        "\n",
        "ConfusionMatrixDisplay(cm, display_labels=['Not Churned', 'Churned']).plot(cmap=plt.cm.Blues, colorbar=False)\n",
        "plt.title('Confusion Matrix');"
      ],
      "metadata": {
        "id": "WjDa4BdvRAJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**we will use Classic ML algorithm**"
      ],
      "metadata": {
        "id": "FQZDot_rYA1W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**XGBOOST classifier without defining optimization parameters**"
      ],
      "metadata": {
        "id": "5z-fkXeqYQE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf=XGBClassifier()\n",
        "clf.fit(X,y)\n",
        "pred1=clf.predict(X)\n",
        "cm=confusion_matrix(y,pred1)\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "scores = {\n",
        "        'ROC-AUC': roc_auc_score(y, pred1),\n",
        "        'Accuracy': accuracy_score(y, pred1),\n",
        "        'F1': f1_score(y, pred1),\n",
        "        'Precision': precision_score(y, pred1),\n",
        "        'Sensitivity (TPR / Recall)': recall_score(y, pred1),\n",
        "        'Specificity (TNR)': tn / (tn+fp)\n",
        "    }\n",
        "\n",
        "print('Testing Set Scores:')\n",
        "for metric, score in scores.items():\n",
        "    print(f'- {metric}: {score:.4f}')\n",
        "    print()\n",
        "\n",
        "ConfusionMatrixDisplay(cm, display_labels=['Not Churned', 'Churned']).plot(cmap=plt.cm.Blues, colorbar=False)\n",
        "plt.title('Confusion Matrix');"
      ],
      "metadata": {
        "id": "kLLjaE-0UU1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2,stratify=y, random_state=42)"
      ],
      "metadata": {
        "id": "XUHokOy2bXXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**XGBOOST classifier without defining optimization parameters. we are going to use bayesian optimisation to find best parameters and use StratifiedKFold cross validation**"
      ],
      "metadata": {
        "id": "A8PaklCsYtdu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stratified_kfold = StratifiedKFold(n_splits=5)\n",
        "\n",
        "def xgb_cv(learning_rate, n_estimators, scale_pos_weight, max_depth, gamma, subsample, colsample_bytree, reg_lambda):\n",
        "    \"\"\"\n",
        "    Calculate cross-validated ROC AUC score for an XGBoost classifier with given hyperparameters.\n",
        "    Returns: Mean ROC AUC score of the cross-validated model (float).\n",
        "    \"\"\"\n",
        "    model = XGBClassifier(scale_pos_weight=scale_pos_weight,\n",
        "                          max_depth=int(max_depth),\n",
        "                          gamma=gamma,\n",
        "                          subsample=subsample,\n",
        "                          colsample_bytree=colsample_bytree,\n",
        "                          reg_lambda=reg_lambda,\n",
        "                          learning_rate=learning_rate,\n",
        "                          n_estimators=int(n_estimators),\n",
        "                          random_state=42,\n",
        "                          eval_metric='auc')\n",
        "    return np.mean(cross_val_score(model, X_train, y_train, cv=stratified_kfold, scoring='roc_auc'))\n",
        "\n",
        "def optimize_xgb():\n",
        "    '''\n",
        "    Optimize hyperparameters for an XGBoost classifier using Bayesian Optimization.\n",
        "    Returns: Dictionary containing the best hyperparameters found by the optimization process.\n",
        "    '''\n",
        "    def xgb_crossval(learning_rate, n_estimators, scale_pos_weight, max_depth, gamma, subsample, colsample_bytree, reg_lambda):\n",
        "        '''\n",
        "        Function to be maximized using Bayesian Optimization.\n",
        "        '''\n",
        "        return xgb_cv(learning_rate, n_estimators, scale_pos_weight, max_depth, gamma, subsample, colsample_bytree, reg_lambda)\n",
        "\n",
        "    optimizer = BayesianOptimization(\n",
        "        f=xgb_crossval,\n",
        "        pbounds={\n",
        "            'scale_pos_weight': (3.6, 3.6),\n",
        "            'max_depth': (3, 3),\n",
        "            'gamma': (5.4, 5.4),\n",
        "            'subsample': (1, 1),\n",
        "            'colsample_bytree': (0.4, 0.4),\n",
        "            'reg_lambda': (14, 14),\n",
        "            'learning_rate': (0.07, 0.07),\n",
        "            'n_estimators':(240, 240)\n",
        "        },\n",
        "        random_state=42,\n",
        "    )\n",
        "    optimizer.maximize(n_iter=20)\n",
        "    return optimizer.max\n",
        "\n",
        "best_params = optimize_xgb()['params']\n",
        "print('Best Hyperparameters found by Bayesian Optimization:\\n', best_params, '\\n')\n",
        "\n",
        "# Train the XGBoost classifier with the best hyperparameters\n",
        "best_xgb = XGBClassifier(\n",
        "    scale_pos_weight=best_params['scale_pos_weight'],\n",
        "    max_depth=int(best_params['max_depth']),\n",
        "    gamma=best_params['gamma'],\n",
        "    subsample=best_params['subsample'],\n",
        "    colsample_bytree=best_params['colsample_bytree'],\n",
        "    reg_lambda=best_params['reg_lambda'],\n",
        "    learning_rate=best_params['learning_rate'],\n",
        "    n_estimators=int(best_params['n_estimators']),\n",
        "    random_state=42\n",
        ")\n",
        "best_xgb.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "jnDzciCReLpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_results(model):\n",
        "    '''\n",
        "    Calculate and print various performance metrics based on the predictions made by the model on the test set.\n",
        "\n",
        "    Parameters:\n",
        "    model: The trained machine learning model\n",
        "\n",
        "    Returns: None\n",
        "    '''\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_score = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "    scores = {\n",
        "        'ROC-AUC': roc_auc_score(y_test, y_score),\n",
        "        'Accuracy': accuracy_score(y_test, y_pred),\n",
        "        'F1': f1_score(y_test, y_pred),\n",
        "        'Precision': precision_score(y_test, y_pred),\n",
        "        'Sensitivity (TPR / Recall)': recall_score(y_test, y_pred),\n",
        "        'Specificity (TNR)': tn / (tn+fp)\n",
        "    }\n",
        "\n",
        "    print('Testing Set Scores:')\n",
        "    for metric, score in scores.items():\n",
        "        print(f'- {metric}: {score:.4f}')\n",
        "    print()\n",
        "\n",
        "    ConfusionMatrixDisplay(cm, display_labels=['Not Churned', 'Churned']).plot(cmap=plt.cm.Blues, colorbar=False)\n",
        "    plt.title('Confusion Matrix');"
      ],
      "metadata": {
        "id": "TBldZHHJftLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_results(best_xgb)"
      ],
      "metadata": {
        "id": "DSKso8QueUpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data is small so Classical ML algorithm is better than deep neural networks**"
      ],
      "metadata": {
        "id": "bkkwn4SSZEOE"
      }
    }
  ]
}